{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2: Data Processing\n",
    "\n",
    "## Task 1\n",
    "This coursework will assess your understanding of using NoSQL to store and retrieve data.  You will perform operations on data from the Enron email dataset in a MongoDB database, and write a report detailing the suitability of different types of databases for data science applications.  You will be required to run code to answer the given questions in the Jupyter notebook provided, and write a report describing alternative approaches to using MongoDB.\n",
    "\n",
    "Download the JSON version of the Enron data (using the “Download as zip” to download the data file from http://edshare.soton.ac.uk/19548/, the file is about 380MB) and import into a collection called messages in a database called enron.  You do not need to set up any authentication.  In the Jupyter notebook provided, perform the following tasks, using the Python PyMongo library.\n",
    "\n",
    "Answers should be efficient in terms of speed.  Answers which are less efficient will not get full marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)\n",
    "Write a function which returns a MongoDB connection object to the \"messages\" collection. [4 points] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "get_collection",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'local']\n"
     ]
    }
   ],
   "source": [
    "def get_collection():\n",
    "    \"\"\"\n",
    "    Connects to the server, and returns a collection object\n",
    "    of the `messages` collection in the `enron` database\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('mongodb://localhost:27017')\n",
    "    print(client.list_database_names())\n",
    "    db=client.enron\n",
    "    collection=db.messages\n",
    "    return None\n",
    "\n",
    "get_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)\n",
    "\n",
    "Write a function which returns the amount of emails in the messages collection in total. [4 points] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "get_amount_of_messages",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_amount_of_messages(collection):\n",
    "    \"\"\"\n",
    "    :param collection A PyMongo collection object\n",
    "    :return the amount of documents in the collection\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) \n",
    "\n",
    "Write a function which returns each person who was BCCed on an email.  Include each person only once, and display only their name according to the X-To header. [4 points] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "get_bcced_people",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_bcced_people(collection):\n",
    "    \"\"\"\n",
    "    :param collection A PyMongo collection object\n",
    "    :return the names of the people who have received an email by BCC\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4)\n",
    "\n",
    "Write a function with parameter subject, which gets all emails in a thread with that parameter, and orders them by date (ascending). “An email thread is an email message that includes a running list of all the succeeding replies starting with the original email.”, check for detail descriptions at https://www.techopedia.com/definition/1503/email-thread [4 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "get_emails_in_thread",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_emails_in_thread(collection, subject):\n",
    "    \"\"\"\n",
    "    :param collection A PyMongo collection object\n",
    "    :return All emails in the thread with that subject\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE    \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5)\n",
    "\n",
    "Write a function which returns the percentage of emails sent on a weekend (i.e., Saturday and Sunday) as a `float` between 0 and 1. [6 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "get_percentage_sent_on_weekend",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_percentage_sent_on_weekend(collection):\n",
    "    \"\"\"\n",
    "    :param collection A PyMongo collection object\n",
    "    :return A float between 0 and 1\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6)\n",
    "\n",
    "Write a function with parameter limit. The function should return for each email account: the number of emails sent, the number of emails received, and the total number of emails (sent and received). Use the following format: [{\"contact\": \"michael.simmons@enron.com\", \"from\": 42, \"to\": 92, \"total\": 134}] and the information contained in the To, From, and Cc headers. Sort the output in descending order by the total number of emails. Use the parameter limit to specify the number of results to be returned. If limit is null, the function should return all results. If limit is higher than null, the function should return the number of results specified as limit. limit cannot take negative values. [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "get_emails_between_contacts",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_emails_between_contacts(collection, limit):\n",
    "    \"\"\"\n",
    "    Shows the communications between contacts\n",
    "    Sort by the descending order of total emails using the To, From, and Cc headers.\n",
    "    :param `collection` A PyMongo collection object    \n",
    "    :param `limit` An integer specifying the amount to display, or\n",
    "    if null will display all outputs\n",
    "    :return A list of objects of the form:\n",
    "    [{\n",
    "        'contact': <<Another email address>>\n",
    "        'from': \n",
    "        'to': \n",
    "        'total': \n",
    "    },{.....}]\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7)\n",
    "Write a function to find out the number of senders who were also direct receivers. Direct receiver means the email is sent to the person directly, not via cc or bcc. [4 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_from_to_people(collection):\n",
    "    \"\"\"\n",
    "    :param collection A PyMongo collection object\n",
    "    :return the NUMBER of the people who have sent emails and received emails as direct receivers.\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8)\n",
    "Write a function with parameters start_date and end_date, which returns the number of email messages that have been sent between those specified dates, including start_date and end_date [4 points] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_emails_between_dates(collection, start_date, end_date):\n",
    "    \"\"\"\n",
    "    :param collection A PyMongo collection object\n",
    "    :return All emails between the specified start_date and end_date\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE    \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "This task will assess your ability to use the Hadoop Streaming API and MapReduce to process data. For each of the questions below, you are expected to write two python scripts, one for the Map phase and one for the Reduce phase. You are also expected to provide the correct parameters to the `hadoop` command to run the MapReduce process. Write down your answers in the specified cells below.\n",
    "\n",
    "To get started, you need to download and unzip the YouTube dataset (available at http://edshare.soton.ac.uk/19547/) onto the machine where you have Hadoop installed (this should be the virtual machine provided).\n",
    "\n",
    "To help you, `%%writefile` has been added to the top of the cells, automatically writing them to \"mapper.py\" and \"reducer.py\" respectively when the cells are run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) \n",
    "Using Youtube01-Psy.csv, find the hourly interval in which most spam was sent. The output should be in the form of a single key-value pair, where the value is a datetime at the start of the hour with the highest number of spam comments. [9 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "#Answer for mapper.py\n",
    "import csv\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "lines = sys.stdin.readlines()\n",
    "csvreader = csv.reader(lines)\n",
    "datetimes=[row[2] for row in csvreader]  #get the \"DATE\" column\n",
    "datetime_token_prefix=[]\n",
    "date_reg='\\d{4}[-/]\\d{2}[-/]\\d{2}[T]'\n",
    "date_reg_exp=re.compile(date_reg)\n",
    "for datetime in datetimes:\n",
    "    datetime_tokens= re.split(r':', datetime)\n",
    "    for datetime_token in datetime_tokens: \n",
    "#         print(datetime_token  + \"\\t1\") \n",
    "#         if(string.find(datetime_token,'T')!=-1):\n",
    "        if(re.match(date_reg_exp,datetime_token)):\n",
    "            datetime_token_prefix.append(datetime_token)\n",
    "\n",
    "for token in datetime_token_prefix:\n",
    "     print(token  + \"\\t1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "#Answer for reducer.py\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "# test list\n",
    "datetime_input_pairs=[\n",
    "    '2013-11-07T06\t1',\n",
    "    '2013-11-07T06\t1',\n",
    "    '2014-11-03T20\t1',\n",
    "    '2014-11-08T15\t1',\n",
    "    '2014-11-08T15\t1',\n",
    "    '2014-11-08T15\t1',\n",
    "    '2014-11-06T24\t1']\n",
    "\n",
    "datetime_input_pairs = sys.stdin.readlines()\n",
    "def fun_get_datetime_with_most_spam(datetime_input_pairs):\n",
    "    datetime_accumulator = defaultdict(lambda: 0)\n",
    "    datetime_with_most_spam_dict=defaultdict(lambda: 0)\n",
    "    for row in datetime_input_pairs:\n",
    "        datetime_key_value_pair = row.split(\"\\t\", 1)  #######row is string, split every row in input_pairs into 2(second parameter 1+1) parts\n",
    "\n",
    "        if len(datetime_key_value_pair) != 2:\n",
    "            continue\n",
    "\n",
    "        word = datetime_key_value_pair[0]\n",
    "        count = int(datetime_key_value_pair[1].strip()) #exception\n",
    "\n",
    "        datetime_accumulator[word] = datetime_accumulator[word] + count\n",
    "\n",
    "    for (key, value) in  datetime_accumulator.items():\n",
    "        print(key + \"\\t\" + str(value))\n",
    "    \n",
    "    datetime_with_most_spam=max(datetime_accumulator,key=datetime_accumulator.get)  #exception  ##just can find one\n",
    "    datetime_with_most_spam_dict[\"datetime_with_most_spam_dict\"]=datetime_with_most_spam+str(\":00:00\")\n",
    "    return datetime_with_most_spam_dict\n",
    "    \n",
    "\n",
    "datetime_with_most_spam_dict=fun_get_datetime_with_most_spam(datetime_input_pairs)\n",
    "for (key, value) in  datetime_with_most_spam_dict.items():\n",
    "        print(key + \"\\t\" + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -r output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenJDK Server VM warning: You have loaded library /opt/hadoop-2.8.5/lib/native/libhadoop.so.1.0.0 which might have disabled stack guard. The VM will try to fix the stack guard now.\n",
      "It's highly recommended that you fix the library with 'execstack -c <libfile>', or link it with '-z noexecstack'.\n",
      "18/12/05 16:46:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "18/12/05 16:46:06 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "18/12/05 16:46:06 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "18/12/05 16:46:06 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "18/12/05 16:46:06 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "18/12/05 16:46:06 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "18/12/05 16:46:06 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local769239196_0001\n",
      "18/12/05 16:46:07 INFO mapred.LocalDistributedCacheManager: Localized file:/home/comp6235/Notebooks/mapper.py as file:/tmp/hadoop-comp6235/mapred/local/1544028366874/mapper.py\n",
      "18/12/05 16:46:07 INFO mapred.LocalDistributedCacheManager: Localized file:/home/comp6235/Notebooks/reducer.py as file:/tmp/hadoop-comp6235/mapred/local/1544028366875/reducer.py\n",
      "18/12/05 16:46:07 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "18/12/05 16:46:07 INFO mapreduce.Job: Running job: job_local769239196_0001\n",
      "18/12/05 16:46:07 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "18/12/05 16:46:07 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "18/12/05 16:46:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/05 16:46:07 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/05 16:46:07 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "18/12/05 16:46:07 INFO mapred.LocalJobRunner: Starting task: attempt_local769239196_0001_m_000000_0\n",
      "18/12/05 16:46:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/05 16:46:07 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/05 16:46:07 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "18/12/05 16:46:07 INFO mapred.MapTask: Processing split: file:/home/comp6235/Notebooks/Youtube01-Psy.csv:0+57438\n",
      "18/12/05 16:46:07 INFO mapred.MapTask: numReduceTasks: 1\n",
      "18/12/05 16:46:07 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "18/12/05 16:46:07 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "18/12/05 16:46:07 INFO mapred.MapTask: soft limit at 83886080\n",
      "18/12/05 16:46:07 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "18/12/05 16:46:07 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "18/12/05 16:46:07 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "18/12/05 16:46:07 INFO streaming.PipeMapRed: PipeMapRed exec [/home/comp6235/Notebooks/././mapper.py]\n",
      "18/12/05 16:46:07 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "18/12/05 16:46:07 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "18/12/05 16:46:07 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "18/12/05 16:46:07 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "18/12/05 16:46:07 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "18/12/05 16:46:07 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "18/12/05 16:46:07 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "18/12/05 16:46:07 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "18/12/05 16:46:07 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "18/12/05 16:46:07 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "18/12/05 16:46:07 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "18/12/05 16:46:07 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "18/12/05 16:46:08 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/05 16:46:08 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/05 16:46:08 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/05 16:46:08 INFO streaming.PipeMapRed: Records R/W=351/1\n",
      "18/12/05 16:46:08 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "18/12/05 16:46:08 INFO streaming.PipeMapRed: mapRedFinished\n",
      "18/12/05 16:46:08 INFO mapred.LocalJobRunner: \n",
      "18/12/05 16:46:08 INFO mapred.MapTask: Starting flush of map output\n",
      "18/12/05 16:46:08 INFO mapred.MapTask: Spilling map output\n",
      "18/12/05 16:46:08 INFO mapred.MapTask: bufstart = 0; bufend = 5600; bufvoid = 104857600\n",
      "18/12/05 16:46:08 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213000(104852000); length = 1397/6553600\n",
      "18/12/05 16:46:08 INFO mapred.MapTask: Finished spill 0\n",
      "18/12/05 16:46:08 INFO mapred.Task: Task:attempt_local769239196_0001_m_000000_0 is done. And is in the process of committing\n",
      "18/12/05 16:46:08 INFO mapred.LocalJobRunner: Records R/W=351/1\n",
      "18/12/05 16:46:08 INFO mapred.Task: Task 'attempt_local769239196_0001_m_000000_0' done.\n",
      "18/12/05 16:46:08 INFO mapred.Task: Final Counters for attempt_local769239196_0001_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=193626\n",
      "\t\tFILE: Number of bytes written=524097\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=351\n",
      "\t\tMap output records=350\n",
      "\t\tMap output bytes=5600\n",
      "\t\tMap output materialized bytes=6306\n",
      "\t\tInput split bytes=99\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=350\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=35\n",
      "\t\tTotal committed heap usage (bytes)=137433088\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=57438\n",
      "18/12/05 16:46:08 INFO mapred.LocalJobRunner: Finishing task: attempt_local769239196_0001_m_000000_0\n",
      "18/12/05 16:46:08 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "18/12/05 16:46:08 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "18/12/05 16:46:08 INFO mapred.LocalJobRunner: Starting task: attempt_local769239196_0001_r_000000_0\n",
      "18/12/05 16:46:08 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/05 16:46:08 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/05 16:46:08 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "18/12/05 16:46:08 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1372205\n",
      "18/12/05 16:46:08 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "18/12/05 16:46:08 INFO reduce.EventFetcher: attempt_local769239196_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "18/12/05 16:46:08 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local769239196_0001_m_000000_0 decomp: 6302 len: 6306 to MEMORY\n",
      "18/12/05 16:46:08 INFO reduce.InMemoryMapOutput: Read 6302 bytes from map-output for attempt_local769239196_0001_m_000000_0\n",
      "18/12/05 16:46:08 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 6302, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->6302\n",
      "18/12/05 16:46:08 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "18/12/05 16:46:08 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "18/12/05 16:46:08 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "18/12/05 16:46:08 INFO mapred.Merger: Merging 1 sorted segments\n",
      "18/12/05 16:46:08 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 6286 bytes\n",
      "18/12/05 16:46:08 INFO reduce.MergeManagerImpl: Merged 1 segments, 6302 bytes to disk to satisfy reduce memory limit\n",
      "18/12/05 16:46:08 INFO reduce.MergeManagerImpl: Merging 1 files, 6306 bytes from disk\n",
      "18/12/05 16:46:08 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "18/12/05 16:46:08 INFO mapred.Merger: Merging 1 sorted segments\n",
      "18/12/05 16:46:08 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 6286 bytes\n",
      "18/12/05 16:46:08 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "18/12/05 16:46:08 INFO streaming.PipeMapRed: PipeMapRed exec [/home/comp6235/Notebooks/././reducer.py]\n",
      "18/12/05 16:46:08 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "18/12/05 16:46:08 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "18/12/05 16:46:08 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/05 16:46:08 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/05 16:46:08 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/05 16:46:08 INFO mapreduce.Job: Job job_local769239196_0001 running in uber mode : false\n",
      "18/12/05 16:46:08 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/12/05 16:46:08 INFO streaming.PipeMapRed: Records R/W=350/1\n",
      "18/12/05 16:46:08 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "18/12/05 16:46:08 INFO streaming.PipeMapRed: mapRedFinished\n",
      "18/12/05 16:46:08 INFO mapred.Task: Task:attempt_local769239196_0001_r_000000_0 is done. And is in the process of committing\n",
      "18/12/05 16:46:08 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "18/12/05 16:46:08 INFO mapred.Task: Task attempt_local769239196_0001_r_000000_0 is allowed to commit now\n",
      "18/12/05 16:46:08 INFO output.FileOutputCommitter: Saved output of task 'attempt_local769239196_0001_r_000000_0' to file:/home/comp6235/Notebooks/output/_temporary/0/task_local769239196_0001_r_000000\n",
      "18/12/05 16:46:08 INFO mapred.LocalJobRunner: Records R/W=350/1 > reduce\n",
      "18/12/05 16:46:08 INFO mapred.Task: Task 'attempt_local769239196_0001_r_000000_0' done.\n",
      "18/12/05 16:46:08 INFO mapred.Task: Final Counters for attempt_local769239196_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=206270\n",
      "\t\tFILE: Number of bytes written=533380\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=181\n",
      "\t\tReduce shuffle bytes=6306\n",
      "\t\tReduce input records=350\n",
      "\t\tReduce output records=182\n",
      "\t\tSpilled Records=350\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tTotal committed heap usage (bytes)=137433088\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2977\n",
      "18/12/05 16:46:08 INFO mapred.LocalJobRunner: Finishing task: attempt_local769239196_0001_r_000000_0\n",
      "18/12/05 16:46:08 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "18/12/05 16:46:09 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/12/05 16:46:09 INFO mapreduce.Job: Job job_local769239196_0001 completed successfully\n",
      "18/12/05 16:46:09 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=399896\n",
      "\t\tFILE: Number of bytes written=1057477\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=351\n",
      "\t\tMap output records=350\n",
      "\t\tMap output bytes=5600\n",
      "\t\tMap output materialized bytes=6306\n",
      "\t\tInput split bytes=99\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=181\n",
      "\t\tReduce shuffle bytes=6306\n",
      "\t\tReduce input records=350\n",
      "\t\tReduce output records=182\n",
      "\t\tSpilled Records=700\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=40\n",
      "\t\tTotal committed heap usage (bytes)=274866176\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=57438\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2977\n",
      "18/12/05 16:46:09 INFO streaming.StreamJob: Output directory: output\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Hadoop command to run the map reduce.\n",
    "\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "-files    mapper.py,reducer.py \\\n",
    "-input    Youtube01-Psy.csv \\\n",
    "-mapper   ./mapper.py \\\n",
    "-reducer  ./reducer.py \\\n",
    "-output output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-11-08T17\t1\n",
      "2014-11-12T05\t2\n",
      "2014-01-19T08\t1\n",
      "2014-11-08T11\t4\n",
      "2014-11-08T10\t9\n",
      "2014-11-08T13\t2\n",
      "2014-01-19T00\t3\n",
      "2014-11-08T15\t3\n",
      "2014-11-12T07\t2\n",
      "2013-11-10T16\t1\n",
      "2014-11-08T01\t1\n",
      "2014-11-12T01\t2\n",
      "2014-11-08T07\t3\n",
      "2015-06-05T14\t1\n",
      "2014-11-12T06\t1\n",
      "2015-06-05T18\t1\n",
      "2014-11-04T20\t1\n",
      "2014-11-04T22\t1\n",
      "2014-01-19T16\t2\n",
      "2014-01-19T17\t2\n",
      "2014-01-19T10\t1\n",
      "2014-11-07T23\t3\n",
      "2014-01-19T13\t1\n",
      "2014-11-08T02\t5\n",
      "2014-11-08T03\t9\n",
      "2014-11-08T00\t3\n",
      "2014-11-07T22\t7\n",
      "2014-11-08T06\t5\n",
      "2014-01-19T19\t1\n",
      "2014-11-08T04\t4\n",
      "2014-11-08T05\t5\n",
      "2014-11-12T00\t1\n",
      "2014-11-02T18\t1\n",
      "2014-11-02T17\t1\n",
      "2014-11-02T16\t1\n",
      "2014-11-02T15\t1\n",
      "2014-11-02T14\t2\n",
      "2014-11-02T12\t1\n",
      "2014-11-13T22\t1\n",
      "2014-11-13T23\t1\n",
      "2014-11-07T14\t3\n",
      "2014-11-07T15\t5\n",
      "2014-11-07T16\t1\n",
      "2015-05-23T13\t1\n",
      "2014-11-12T17\t1\n",
      "2014-11-12T14\t1\n",
      "2014-11-12T15\t1\n",
      "2014-11-06T22\t3\n",
      "2014-11-06T23\t2\n",
      "2014-11-06T20\t1\n",
      "2014-11-12T11\t1\n",
      "2013-12-01T04\t1\n",
      "2013-12-01T03\t1\n",
      "2013-12-01T01\t1\n",
      "2014-11-11T22\t2\n",
      "2013-11-26T02\t1\n",
      "2014-11-02T00\t1\n",
      "2014-11-02T01\t1\n",
      "2014-01-19T04\t2\n",
      "2014-11-02T05\t1\n",
      "2014-11-05T07\t3\n",
      "2014-11-05T06\t1\n",
      "2014-11-05T01\t1\n",
      "2014-11-05T00\t1\n",
      "2014-11-07T18\t4\n",
      "2014-11-05T08\t2\n",
      "2014-11-07T19\t7\n",
      "2014-11-08T12\t1\n",
      "2014-11-06T19\t3\n",
      "2014-11-06T18\t6\n",
      "2013-11-09T08\t1\n",
      "2014-11-06T13\t1\n",
      "2014-11-12T20\t2\n",
      "2014-11-06T17\t2\n",
      "2014-11-06T16\t1\n",
      "2014-11-06T15\t2\n",
      "2014-11-06T14\t1\n",
      "2014-01-20T21\t1\n",
      "2014-01-20T20\t3\n",
      "2014-01-20T22\t1\n",
      "2014-11-13T21\t1\n",
      "2014-11-11T19\t1\n",
      "2014-11-12T23\t1\n",
      "2013-12-27T23\t1\n",
      "2014-11-05T16\t2\n",
      "2014-11-05T17\t1\n",
      "2014-11-05T15\t6\n",
      "2014-11-05T10\t1\n",
      "2014-11-07T21\t2\n",
      "2014-11-07T20\t6\n",
      "2014-11-05T18\t3\n",
      "2014-11-05T19\t1\n",
      "2014-11-06T04\t3\n",
      "2014-11-06T05\t1\n",
      "2014-11-06T01\t2\n",
      "2014-11-06T02\t2\n",
      "2014-11-06T03\t1\n",
      "2014-11-13T02\t1\n",
      "2014-11-06T09\t3\n",
      "2014-11-03T21\t1\n",
      "2014-11-03T20\t2\n",
      "2014-11-03T23\t3\n",
      "2014-11-03T22\t2\n",
      "2014-11-09T02\t1\n",
      "2014-11-09T04\t1\n",
      "2014-11-07T02\t1\n",
      "2014-11-03T17\t1\n",
      "2014-01-21T13\t1\n",
      "2014-11-07T01\t3\n",
      "2013-12-25T19\t1\n",
      "2014-11-13T15\t2\n",
      "2014-11-05T22\t3\n",
      "2014-11-05T21\t4\n",
      "2014-11-05T20\t3\n",
      "2014-11-07T07\t2\n",
      "2014-11-07T05\t1\n",
      "2014-01-20T09\t1\n",
      "2014-01-20T06\t2\n",
      "2014-01-20T04\t1\n",
      "2014-01-20T02\t1\n",
      "2014-11-07T04\t3\n",
      "2014-11-10T13\t1\n",
      "2014-01-21T08\t1\n",
      "2014-01-21T09\t1\n",
      "2014-01-21T02\t2\n",
      "2014-01-21T03\t2\n",
      "2014-01-21T07\t1\n",
      "2014-11-13T06\t1\n",
      "2014-11-13T07\t1\n",
      "2014-11-07T09\t2\n",
      "2014-11-07T08\t1\n",
      "2014-11-04T19\t1\n",
      "2014-11-04T18\t3\n",
      "2014-11-13T00\t2\n",
      "2014-11-04T14\t1\n",
      "2013-12-23T12\t1\n",
      "2014-11-04T11\t1\n",
      "2014-11-13T17\t1\n",
      "2014-11-04T13\t2\n",
      "2014-11-04T12\t1\n",
      "2014-01-20T18\t1\n",
      "2013-11-29T02\t1\n",
      "2014-01-25T19\t1\n",
      "2013-11-07T12\t1\n",
      "2014-01-20T10\t1\n",
      "2014-01-27T19\t1\n",
      "2013-11-29T00\t1\n",
      "2014-01-20T13\t1\n",
      "2014-01-20T15\t2\n",
      "2014-01-20T16\t2\n",
      "2014-01-20T17\t4\n",
      "2013-11-28T12\t1\n",
      "2014-11-14T13\t3\n",
      "2013-11-28T16\t2\n",
      "2013-11-28T17\t3\n",
      "2014-11-14T00\t1\n",
      "2013-11-28T18\t1\n",
      "2013-11-28T19\t1\n",
      "2014-11-02T22\t1\n",
      "2014-11-02T23\t1\n",
      "2014-11-02T20\t1\n",
      "2014-11-07T17\t4\n",
      "2014-11-07T12\t4\n",
      "2014-11-07T13\t4\n",
      "2014-11-04T07\t1\n",
      "2014-11-04T02\t2\n",
      "2014-11-04T03\t1\n",
      "2014-11-04T00\t2\n",
      "2014-11-03T19\t1\n",
      "2013-11-07T06\t1\n",
      "2014-11-10T17\t1\n",
      "2014-11-03T14\t1\n",
      "2014-11-03T16\t1\n",
      "2013-11-27T21\t1\n",
      "2013-11-28T23\t1\n",
      "2014-11-08T08\t1\n",
      "2013-11-28T21\t3\n",
      "2014-11-14T11\t1\n",
      "2014-11-14T12\t2\n",
      "2014-11-08T09\t7\n",
      "2014-11-12T09\t3\n",
      "datetime_with_most_spam_dict\t2014-11-08T10:00:00\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd output\n",
    "cat part-00000\n",
    "#Expected key-value output format:\n",
    "#hour_with_most_spam\t\"2013-11-10T10:00:00\"\n",
    "#Additional key-value pairs are acceptable, as long as the hour_with_most_spam pair is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) \n",
    "Find all comments associated with a username (the AUTHOR field). Return a JSON array of all comments associated with that username. (This should use the data from all 5 data files: Psy, KatyPerry, LMFAO, Eminem, Shakira) [11 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "#Answer for mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "#Answer for reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#Hadoop command to run the map reduce.\n",
    "\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "-files    \\\n",
    "-input    \\\n",
    "-mapper   \\\n",
    "-reducer  \\\n",
    "-output output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Expected key-value output format:\n",
    "#John Smith\t[\"Comment 1\", \"Comment 2\", \"Comment 3\", \"etc.\"]\n",
    "#Jane Doe\t[\"Comment 1\", \"Comment 2\", \"Comment 3\", \"etc.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
